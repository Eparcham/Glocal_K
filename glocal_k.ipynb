{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"glocal_k.ipynb","provenance":[{"file_id":"1at3AcPm_t-R011A6XaGvtBjHzg8mfcDg","timestamp":1620356766659}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ty3gYQgtnwFA"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nl2tU6kL8Ot3"},"source":["%tensorflow_version 1.x\n","from time import time\n","from scipy.sparse import csc_matrix\n","import tensorflow as tf\n","import numpy as np\n","import h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pcyWCv_WvSKC"},"source":["print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k4A9uU1WloQ2"},"source":["# Data Loader Function"]},{"cell_type":"code","metadata":{"id":"cq3KEUaVo1o3"},"source":["def load_data_ml_100k(path='./', delimiter='\\t'):\n","\n","    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n","    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n","    total = np.concatenate((train, test), axis=0)\n","\n","    n_u = np.unique(total[:,0]).size  # num of users\n","    n_m = np.unique(total[:,1]).size  # num of movies\n","    n_train = train.shape[0]  # num of training ratings\n","    n_test = test.shape[0]  # num of test ratings\n","\n","    train_r = np.zeros((n_m, n_u), dtype='float32')\n","    test_r = np.zeros((n_m, n_u), dtype='float32')\n","\n","    for i in range(n_train):\n","        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n","\n","    for i in range(n_test):\n","        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n","\n","    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n","    test_m = np.greater(test_r, 1e-12).astype('float32')\n","\n","    print('data matrix loaded')\n","    print('num of users: {}'.format(n_u))\n","    print('num of movies: {}'.format(n_m))\n","    print('num of training ratings: {}'.format(n_train))\n","    print('num of test ratings: {}'.format(n_test))\n","\n","    return n_m, n_u, train_r, train_m, test_r, test_m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3e8Xg3us8g7"},"source":["def load_data_ml_1m(path='./', delimiter='::', frac=0.1, seed=1234):\n","\n","    tic = time()\n","    print('reading data...')\n","    data = np.loadtxt(path+'movielens_1m_dataset.dat', skiprows=0, delimiter=delimiter).astype('int32')\n","    print('taken', time() - tic, 'seconds')\n","\n","    n_u = np.unique(data[:,0]).size  # num of users\n","    n_m = np.unique(data[:,1]).size  # num of movies\n","    n_r = data.shape[0]  # num of ratings\n","\n","    udict = {}\n","    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n","        udict[u] = i\n","    mdict = {}\n","    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n","        mdict[m] = i\n","\n","    np.random.seed(seed)\n","    idx = np.arange(n_r)\n","    np.random.shuffle(idx)\n","\n","    train_r = np.zeros((n_m, n_u), dtype='float32')\n","    test_r = np.zeros((n_m, n_u), dtype='float32')\n","\n","    for i in range(n_r):\n","        u_id = data[idx[i], 0]\n","        m_id = data[idx[i], 1]\n","        r = data[idx[i], 2]\n","\n","        if i < int(frac * n_r):\n","            test_r[mdict[m_id], udict[u_id]] = r\n","        else:\n","            train_r[mdict[m_id], udict[u_id]] = r\n","\n","    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n","    test_m = np.greater(test_r, 1e-12).astype('float32')\n","\n","    print('data matrix loaded')\n","    print('num of users: {}'.format(n_u))\n","    print('num of movies: {}'.format(n_m))\n","    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n","    print('num of test ratings: {}'.format(int(frac * n_r)))\n","\n","    return n_m, n_u, train_r, train_m, test_r, test_m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rMjcbLvhtRs"},"source":["def load_matlab_file(path_file, name_field):\n","    \n","    db = h5py.File(path_file, 'r')\n","    ds = db[name_field]\n","\n","    try:\n","        if 'ir' in ds.keys():\n","            data = np.asarray(ds['data'])\n","            ir   = np.asarray(ds['ir'])\n","            jc   = np.asarray(ds['jc'])\n","            out  = csc_matrix((data, ir, jc)).astype(np.float32)\n","    except AttributeError:\n","        out = np.asarray(ds).astype(np.float32).T\n","\n","    db.close()\n","\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6pIUrkza2zv"},"source":["def load_data_monti(path='./'):\n","\n","    M = load_matlab_file(path+'douban_monti_dataset.mat', 'M')\n","    Otraining = load_matlab_file(path+'douban_monti_dataset.mat', 'Otraining') * M\n","    Otest = load_matlab_file(path+'douban_monti_dataset.mat', 'Otest') * M\n","\n","    n_u = M.shape[0]  # num of users\n","    n_m = M.shape[1]  # num of movies\n","    n_train = np.where(Otraining)[0].size  # num of training ratings\n","    n_test = np.where(Otest)[0].size  # num of test ratings\n","\n","    train_r = Otraining.T\n","    test_r = Otest.T\n","\n","    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n","    test_m = np.greater(test_r, 1e-12).astype('float32')\n","\n","    print('data matrix loaded')\n","    print('num of users: {}'.format(n_u))\n","    print('num of movies: {}'.format(n_m))\n","    print('num of training ratings: {}'.format(n_train))\n","    print('num of test ratings: {}'.format(n_test))\n","\n","    return n_m, n_u, train_r, train_m, test_r, test_m"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_8kEkg9mlIW"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"0fkA1WpmipzF"},"source":["# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n","# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n","data_path = '/content/drive/MyDrive/USYD/CIKM/GLocal-K/data'\n","# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijlu0lXQioYM"},"source":["# Select a dataset among 'ML-1M', 'ML-100K', and 'Douban'\n","# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n","dataset = 'ML-1M'\n","# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJqSSY33mgkw"},"source":["# Data Load\n","try:\n","    if dataset == 'ML-100K':\n","        path = data_path + '/MovieLens_100K/'\n","        n_m, n_u, train_r, train_m, test_r, test_m = load_data_ml_100k(path=path, delimiter='\\t')\n","\n","    elif dataset == 'ML-1M':\n","        path = data_path + '/MovieLens_1M/'\n","        n_m, n_u, train_r, train_m, test_r, test_m = load_data_ml_1m(path=path, delimiter='::', frac=0.1, seed=1234)\n","\n","    elif dataset == 'Douban':\n","        path = data_path + '/Douban_monti/'\n","        n_m, n_u, train_r, train_m, test_r, test_m = load_data_monti(path=path)\n","\n","    else:\n","        raise ValueError\n","\n","except ValueError:\n","    print('Error: Unable to load data')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQMtA9yml-gp"},"source":["# Hyperparameter Settings"]},{"cell_type":"code","metadata":{"id":"344bwGB0cWXp"},"source":["# Common hyperparameter settings\n","n_hid = 500\n","n_dim = 5\n","n_layers = 2\n","gk_size = 3\n","\n","# Different hyperparameter settings for each dataset\n","if dataset == 'ML-100K':\n","    lambda_2 = 20.\n","    lambda_s = 0.006\n","    iter_p = 5\n","    iter_f = 5\n","    epoch_p = 30\n","    epoch_f = 60\n","    dot_scale = 1\n","\n","elif dataset == 'ML-1M':\n","    lambda_2 = 70.\n","    lambda_s = 0.018\n","    iter_p = 50\n","    iter_f = 10\n","    epoch_p = 20\n","    epoch_f = 30\n","    dot_scale = 0.5\n","\n","elif dataset == 'Douban':\n","    lambda_2 = 10.\n","    lambda_s = 0.022\n","    iter_p = 5\n","    iter_f = 5\n","    epoch_p = 20\n","    epoch_f = 60\n","    dot_scale = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b94aimX3nAMI"},"source":["R = tf.placeholder(\"float\", [n_m, n_u])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5sWtU4-pmDDT"},"source":["# Network Function"]},{"cell_type":"code","metadata":{"id":"wX2wREO09zde"},"source":["def local_kernel(u, v):\n","\n","    dist = tf.norm(u - v, ord=2, axis=2)\n","    hat = tf.maximum(0., 1. - dist**2)\n","\n","    return hat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c88l9LYr9175"},"source":["def kernel_layer(x, n_hid=n_hid, n_dim=n_dim, activation=tf.nn.sigmoid, lambda_s=lambda_s, lambda_2=lambda_2, name=''):\n","\n","    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n","        W = tf.get_variable('W', [x.shape[1], n_hid])\n","        n_in = x.get_shape().as_list()[1]\n","        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n","        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n","        b = tf.get_variable('b', [n_hid])\n","\n","    w_hat = local_kernel(u, v)\n","    \n","    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n","    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n","    \n","    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n","    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n","\n","    W_eff = W * w_hat  # Local kernelised weight matrix\n","    y = tf.matmul(x, W_eff) + b\n","    y = activation(y)\n","\n","    return y, sparse_reg_term + l2_reg_term"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlb95FmRVATa"},"source":["def global_kernel(input, gk_size):\n","\n","    avg_pooling = tf.reduce_mean(input, axis=1)  # Item (axis=1) based average pooling\n","    avg_pooling = tf.reshape(avg_pooling, [1, -1])\n","    n_kernel = avg_pooling.shape[1].value\n","\n","    conv_kernel = tf.get_variable('conv_kernel', initializer=tf.random.truncated_normal([n_kernel, gk_size**2], stddev=0.1))\n","    gk = tf.matmul(avg_pooling, conv_kernel) * dot_scale  # Scaled dot product\n","    gk = tf.reshape(gk, [gk_size, gk_size, 1, 1])\n","\n","    return gk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTLi_65XzIbH"},"source":["def global_conv(input, W):\n","\n","    input = tf.reshape(input, [1, input.shape[0], input.shape[1], 1])\n","    conv2d = tf.nn.relu(tf.nn.conv2d(input, W, strides=[1,1,1,1], padding='SAME'))\n","\n","    return tf.reshape(conv2d, [conv2d.shape[1], conv2d.shape[2]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8sQCwrSmKG4"},"source":["# Network Instantiation"]},{"cell_type":"markdown","metadata":{"id":"zOtWj1SCo1RW"},"source":["## Pre-training"]},{"cell_type":"code","metadata":{"id":"7teUrgWagpW0"},"source":["y = R\n","reg_losses = None\n","\n","for i in range(n_layers):\n","    y, reg_loss = kernel_layer(y, name=str(i))\n","    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n","\n","pred_p, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n","reg_losses = reg_losses + reg_loss\n","\n","# L2 loss\n","diff = train_m * (train_r - pred_p)\n","sqE = tf.nn.l2_loss(diff)\n","loss_p = sqE + reg_losses\n","\n","optimizer_p = tf.contrib.opt.ScipyOptimizerInterface(loss_p, options={'disp': True, 'maxiter': iter_p, 'maxcor': 10}, method='L-BFGS-B')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4IEBsNhNo4Cj"},"source":["## FIne-tuning"]},{"cell_type":"code","metadata":{"id":"OiTXqnN6zLXQ"},"source":["y = R\n","reg_losses = None\n","\n","for i in range(n_layers):\n","    y, _ = kernel_layer(y, name=str(i))\n","\n","y_dash, _ = kernel_layer(y, n_u, activation=tf.identity, name='out')\n","\n","gk = global_kernel(y_dash, gk_size)  # Global kernel\n","y_hat = global_conv(train_r, gk)  # Global kernel-based rating matrix\n","\n","for i in range(n_layers):\n","    y_hat, reg_loss = kernel_layer(y_hat, name=str(i))\n","    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n","\n","pred_f, reg_loss = kernel_layer(y_hat, n_u, activation=tf.identity, name='out')\n","reg_losses = reg_losses + reg_loss\n","\n","# L2 loss\n","diff = train_m * (train_r - pred_f)\n","sqE = tf.nn.l2_loss(diff)\n","loss_f = sqE + reg_losses\n","\n","optimizer_f = tf.contrib.opt.ScipyOptimizerInterface(loss_f, options={'disp': True, 'maxiter': iter_f, 'maxcor': 10}, method='L-BFGS-B')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXXQjeMxmYEC"},"source":["# Training and Test Loop"]},{"cell_type":"code","metadata":{"id":"UZ35Zoha-Eue"},"source":["time_cumulative = 0\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for i in range(epoch_p):\n","        tic = time()\n","        optimizer_p.minimize(sess, feed_dict={R: train_r})\n","        pre = sess.run(pred_p, feed_dict={R: train_r})\n","\n","        t = time() - tic\n","        time_cumulative += t\n","        \n","        error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n","        error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n","\n","        print('.-^-._' * 12)\n","        print('PRE-TRAINING')\n","        print('Epoch:', i+1, 'test rmse:', np.sqrt(error), 'train rmse:', np.sqrt(error_train))\n","        print('Time:', t, 'seconds')\n","        print('Time cumulative:', time_cumulative, 'seconds')\n","        print('.-^-._' * 12)\n","\n","    for i in range(epoch_f):\n","        tic = time()\n","        optimizer_f.minimize(sess, feed_dict={R: train_r})\n","        pre = sess.run(pred_f, feed_dict={R: train_r})\n","\n","        t = time() - tic\n","        time_cumulative += t\n","        \n","        error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n","        error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n","\n","        if i > 0:\n","            if np.sqrt(error) < best_rmse:\n","                best_epoch = i\n","                best_rmse = np.sqrt(error)\n","        else:\n","            best_epoch = i\n","            best_rmse = np.sqrt(error)\n","\n","        print('.-^-._' * 12)\n","        print('FINE-TUNING')\n","        print('Epoch:', i+1, 'test rmse:', np.sqrt(error), 'train rmse:', np.sqrt(error_train))\n","        print('Time:', t, 'seconds')\n","        print('Time cumulative:', time_cumulative, 'seconds')\n","        print('Best rmse:', best_rmse, '(%d)' % (best_epoch+1))\n","        print('.-^-._' * 12)"],"execution_count":null,"outputs":[]}]}